---
published: false
sources: https://www.youtube.com/watch?v=4Gy_5EQMrB4&index=5&list=PLLIx5ktghjqKtZdfDDyuJrlhC-ICfhVAN&t=0s
---

so the title of my talk is practicing chaos engineering and that goes against the grain of what we really do. because the goal really is resiliency so I'm going to change the title on you and I will explain why. the title is going to change from practice in chaos engineering to practicing resilience engineering. the key thing to understand here is we are not trying to practice inserting chaos in a system, we are trying to practice how resilient we can be when there is chaos in a system. so taking this we will make sure that we keep carry this meaning on through the rest of the slides.

so what are my goals at Walmart so I joined Walmart about 10 months or so ago I used to work at Netflix before so the ideas that I had at Netflix I'm carrying it on to Walmart. the first point that resonates with me and with Walmart as a company is the customer always comes first. which means the customer experience is paramount. anything that impacts the customer experience and causes revenue loss is the first thing we should be fixing. teams own resiliency which means each application team owns the individual -- they are essentially responsible for ensuring that the quality of their product is absolutely the best. we can obviously federate a bunch of rules and we can say here is how you monitor your system here is how you run tests but they own resiliency. 

teams should be encouraged to fail fast which means if you make a mistake you learn about it very quickly. they need to fail often which means you run a lot of game days to figure out exactly where your vulnerabilities are. once you do that my goals are at least fulfilled. 

this is what my team does at Walmart. the role of our team is to make sure that we centralize the best practices. we we provide all the tools that the teams need. we provide all the techniques that they need in order to run game days in order run tests we enforce and facilitate game days by enforce we mean be ensure that there is a cadence applied to every team's natural release policy and if there is vulnerability checks and vulnerability detection that's attached to each release. we also make sure that we create tools that are part of every stage of the CD pipeline. right we don't want someone to just basically say oh "I I just tested it in prod and I'm never going to test it in dev." we want it to be tested all the way from dev to prod. we also monitor acceptable levels of resiliency. the reason we do that is because we want to call out risks to the greater business and you want to say here is where our weakest links are and here is how we will get affected on key business days. for example Thanksgiving right Black Friday right after. 

so I'm gonna skip the slide because every speaker has spoken about this. these are the things that the apps should be resilient to. the most important thing from our perspective, in addition to application dependencies and infrastructure issues, is how are you deployed? which is a question that a lot of people sort of skip over. deployment itself comes with a certain kind of constraint that ensures that you are resilient or not, right? so we wanted to make sure that our teams our app teams have all the tools they need. however what is not clear is what the process will be in order to get from zero to completely resilient right. and most cases people want to get into chaos engineering immediately. so the first couple of weeks someone some teams came to me and said okay chaos engineering let's just go break something. I was like fine do you have a DR playbook a disaster recovery playbook? and they were like what is that and i said go away do your homework. so be we have to stop that right and in order that we reduce the number of teams that just come and say I want to switch off the data center plug and and cause some issues in the edge, we need to have some prerequisites right.

so in order to get through this entire process we established a series of levels.  so instead of saying you are resilient today and not tomorrow we said let's go through some steps let's go through level one through five. as the levels change from the color red to the color green you are more and more your support cost and your revenue loss gets lower as your resilience gets higher. so by the time you reach green, which is all the way at level five, you are causing little over hundred or so dollars 100 or so dollars of support costs. whereas if you are at one you are probably causing hundreds of thousands of dollars in support costs. 

so this is how we motivated teams, so it is a bit of positive reinforcement along with telling them here is why this is important. so the senior management team obviously looks at the left-hand side and the the teams were actually doing this looking at the right-hand side and making sure that they can get from one level to another. so the prerequisites for us was do you have a dr failover playbook? how would you manage all the traffic that is coming in to multiple data centers can you just exist in one and support everything? what are your critical dependencies? so anything that you cannot survive without, those are your critical dependencies. if you cannot function if you cannot provide what the customer needs then that is your critical dependency and you need to design a good fallback against it. you want to have a playbook for what happens when those critical dependencies fail or do not give you what you need.

we need to define non-critical dependencies too. you may have a database that you have as your L2 cache that you are going to, but what is the amount of time after which that also becomes critical? those are things that need to be well defined so that all of your stakeholders all of the people who use your products know exactly what to expect.

so the first thing after completing all of these prerequisites is we wanted everyone to get a checkup. so the team wrote a tool called "the resiliency doctor." the resiliency doctor is literally a debugging tool for an entire application deployment. so no matter where you are deployed we give you a one-page report saying here is your issue if your highly available, here is what your vulnerabilities are. if you are active passive then where are your rules to make sure that the transition is smooth how do you ensure there is low data loss. so this became the first step for every resiliency exercise that we did at Walmart.

so level one
so once you have all of your prerequisites completed, level one was given as a first step for every team to even start educating themselves about what is resiliency. so all of the prerequisites are stored in a well known place. we have agreement on those playbooks. it's not like I write something and then someone else doesn't even know how to run it. we have to make sure that it's in a language that makes sense to everyone. and we end that level one by making sure that we can do a failover exercise manually that verifies that the playbook actually works.

level two is it's again edited, all of these levels are edited. it this level to it the only thing that changes is making sure that you can do failure injection test for all of your dependencies for application dependencies. 

level three is where automation starts kicking in. so we are pushing teams to start doing using tools right tools like gremlin we have internal tools which they can use to basically push that either infrastructure or that applications to fail and then see what the response looks like and make sure that they have good playbooks to fix that and reduce the revenue loss over time.

level four increases the amount of automation.

by the time you are at level five you are completely automated and literally the only support that someone in sre or SiteOps have to give you is a few hours of engineer time to ensure that the right buttons are clicked.

so we have a long way to go obviously these five levels are not something that we can just like jump and and get to. but the reason for having these levels is to ensure that teams know that there is a step ladder to success. they don't need to jump from one level to another. it also means it gives us a greater amount of time to support these teams. if there are teams at level one we know that we can form a certain kind of support group for them, a community for them internally and Walmart has thousands of teams. to create that community internally and create a community of the chaos practitioners we are able to move teams together from like level one to level two to level three and so on. and that's that's really the key thing that we got out of this process.

so what have we seen so far? the the the net effect is that we have more than 50 teams (this is in the last five months or six months since the entire team join), about 50 teams have already passed level two and are reaching level three. there is a consolidated effort or I should say a consolidated educational effort where we have actually found chaos practitioners internally at Walmart who have been able to lead the effort by saying "okay I see something new I want to talk to you about this" and then gathering all these people doing internal meetups, talking to them, and understanding exactly where we want to be. there have been chaos champions internally who have been able to take not just their team, but also a multitude of teams in their pillar together on the resiliency journey. so that has been a big win.

actively using: so we have suffered some outages rights occurred some weeks back there was a massive storm in Texas and there was a huge outage in the Dallas in the data centers. so Microsoft suffered this huge outage and the same thing happened to us too. however there were lots of teams who had already begun using the failover playbooks that they had developed over time so in a way they were prepared. of course everyone wasn't as prepared as them but that helped reduce the kind of disaster that it could have been.

I think the biggest takeaway for me out of this is seeing that teams themselves find that they are empowered. so there is no more silos that exists. in most teams that have been doing this technology work for years and especially in place like Walmart where technology has been growing for the last 30 years. there have been devs, there have been QAs, there have been performance engineers. this was the first time I could see all of these folks come together as a team saying "no this is all us." there was a sense of ownership a sense of passion when it comes to executing resiliency exercises and making sure that Walmart doesn't suffer any kind of revenue loss. it brought people together and I think that was really awesome to see.

I think overall because of all of the influence that Netflix had on me in terms of the freedom and responsibility culture, that's something that I have seen grow at Walmart.  I'm hoping that all of the efforts that we take is something that pushes them in that direction as well. so there is increasingly a culture of accountability, which means if someone is making a change it it behooves them to also note down exactly what it impacts, what the revenue loss would be if something goes down, and what the playbook should be if a dependency that is a new injection -- if that goes down what happens. so it's it's coming naturally to people and I think that's a big win in my book. 

so those are the results so far. additionally what we are trying to do in the future is trying to automate and create tools, and make sure that we can either open-source them, or we can work with the open source community on figuring out the best possible open source model for those tools. so that's going to be the next set of steps that you're going to start working on.

so we have a pretty long way to go. we have started slow there is lots of teams spanning the entire globe and resiliency at Walmart doesn't stay only to the application layer it goes down all the way to the infrastructure and even out-of-band failures. we have had issues where literally a engineer drives to the data center and fixes something so that was new to me but I'm educating myself as I go as well. so we have a pretty long way to go but we are in the right track. there's definitely a lot of help from the chaos community, which is growing every day. and as people are learning new ideas the sense of partnership on different kinds of projects is increasing and I'm hoping that this conference becomes that kind of birthplace for all these new ideas all these new collaborations. that's all I had thank you.